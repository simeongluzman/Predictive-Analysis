{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Summary\n",
    "This code trains a Long Short-Term Memory (LSTM) neural network to predict the next user action based on the user's history of interactions. The input data is taken from a CSV file and consists of a set of user interactions with a learning platform, where each interaction is associated with a user ID, a timestamp, and an action performed by the user.\n",
    "\n",
    "The code pre-processes the input data by extracting features from the timestamp column, one-hot encoding the categorical columns, and normalizing the non-categorical columns. The target variable 'action' is also one-hot encoded. The preprocessed data is then split into training and testing sets, and an LSTM model is trained on the training set to predict the next user action.\n",
    "\n",
    "The model is evaluated on the test set and the accuracy is printed to the console. Additionally, the code provides a function to predict the next n actions for a given user, as well as a function to predict the next n actions for each unique user in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Input Data\n",
    "The code reads the input data from a CSV file using the pd.read_csv function. The dt column is converted to a datetime format, and we drop the columns which we decided not to include in the model. I decided to remove all the 'id' columns except for the user_id because there are directly correlated to other attributes. eg. (step and step_id represent the same information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas dataframe\n",
    "df = pd.read_csv('internship_assignment.csv')\n",
    "\n",
    "# Convert the 'dt' column to a datetime format\n",
    "df['dt'] = pd.to_datetime(df['dt'])\n",
    "\n",
    "# Extract features from 'dt' column\n",
    "df['day_of_week'] = df['dt'].dt.dayofweek\n",
    "df['hour_of_day'] = df['dt'].dt.hour\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['dt', 'selected_track_id', 'selected_project_id', 'step_id'])\n",
    "\n",
    "# Sort the data by 'user_id_hashed' columns in ascending order\n",
    "df = df.sort_values(['user_id_hashed'], ascending=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & One-Hot Encoding\n",
    "\n",
    "The code extracts features from the dt column using the dt.dayofweek and dt.hour functions. These features are added to the input data as new columns.The code one-hot encodes the categorical columns in the input data using the OneHotEncoder class from Scikit-learn. The encoder is fitted on the entire dataset and transforms the categorical columns in one step. The encoded data is then concatenated with the non-categorical columns and the user_id_hashed column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of the categorical column names\n",
    "categorical_columns = ['user_id_hashed', 'learning_goal', 'selected_project', 'topic', 'project', 'project_difficulty', 'step', 'step_difficulty']\n",
    "\n",
    "# Define an instance of the OneHotEncoder class for encoding categorical variables\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform the categorical columns in one step\n",
    "encoded_categorical_data = encoder.fit_transform(df[categorical_columns])\n",
    "\n",
    "# Combine the non-categorical columns and 'user_id_hashed' with the encoded categorical data\n",
    "non_categorical_columns = ['day_of_week', 'hour_of_day']\n",
    "encoded_data = pd.concat([df[non_categorical_columns], df['user_id_hashed'], pd.DataFrame(encoded_categorical_data.toarray())], axis=1)\n",
    "\n",
    "# One-hot encode the target variable 'action'\n",
    "target_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded_target = target_encoder.fit_transform(df[['action']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting & Preprocessing\n",
    "The code splits the preprocessed data into training and testing sets using the train_test_split function from Scikit-learn. The non-categorical columns are normalized using the MinMaxScaler function from Scikit-learn.\n",
    "\n",
    "The target variables are converted to one-hot encoded format using the to_categorical function from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the non-categorical columns using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "encoded_data[['day_of_week', 'hour_of_day']] = scaler.fit_transform(encoded_data[['day_of_week', 'hour_of_day']])\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_data, encoded_target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the sparse matrix to a dense NumPy array and get the index of the non-zero element\n",
    "y_train = y_train.toarray().argmax(axis=1)\n",
    "y_test = y_test.toarray().argmax(axis=1)\n",
    "\n",
    "# Convert the target variables to one-hot encoded format\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "n_actions = y_train.shape[1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation & Training\n",
    "The code creates an LSTM model using the Keras Sequential class. The model consists of three LSTM layers with increasing numbers of hidden units, followed by a dense output layer with a softmax activation function. The model is compiled using the Adam optimizer and categorical cross-entropy loss function.\n",
    "\n",
    "The input data is reshaped to be in a 3D format (samples, timesteps, features), as required by the LSTM model. The model is trained on the training data using the fit function. The training history is stored in the history variable.\n",
    "\n",
    "There is an error under the first part of training because I decided to interrupt the process early. The accuracy results were getting good, and I didn't want to run the model for an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "762/762 [==============================] - 40s 48ms/step - loss: 1.6860 - accuracy: 0.4638 - val_loss: 1.3839 - val_accuracy: 0.5486\n",
      "Epoch 2/50\n",
      "762/762 [==============================] - 37s 48ms/step - loss: 1.2288 - accuracy: 0.6006 - val_loss: 1.2875 - val_accuracy: 0.5878\n",
      "Epoch 3/50\n",
      "762/762 [==============================] - 36s 47ms/step - loss: 1.0802 - accuracy: 0.6393 - val_loss: 1.2476 - val_accuracy: 0.6022\n",
      "Epoch 4/50\n",
      "762/762 [==============================] - 35s 46ms/step - loss: 0.9543 - accuracy: 0.6739 - val_loss: 1.2373 - val_accuracy: 0.6201\n",
      "Epoch 5/50\n",
      "762/762 [==============================] - 36s 47ms/step - loss: 0.8420 - accuracy: 0.7043 - val_loss: 1.1986 - val_accuracy: 0.6227\n",
      "Epoch 6/50\n",
      "762/762 [==============================] - 37s 48ms/step - loss: 0.7534 - accuracy: 0.7330 - val_loss: 1.2337 - val_accuracy: 0.6252\n",
      "Epoch 7/50\n",
      "527/762 [===================>..........] - ETA: 11s - loss: 0.6831 - accuracy: 0.7518"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m X_test_reshaped \u001b[39m=\u001b[39m X_test\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape((X_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, X_test\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[1;32m     14\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train_reshaped, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test_reshaped, y_test), verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)  \u001b[39m# Increase epochs to 50\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Increase the number of hidden units and LSTM layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, activation='relu', input_shape=(1, n_features), return_sequences=True))\n",
    "model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(64, activation='relu'))\n",
    "model.add(Dense(n_actions, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the input data to be 3D, as required by the LSTM model (samples, timesteps, features)\n",
    "X_train_reshaped = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_reshaped = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train, epochs=50, validation_data=(X_test_reshaped, y_test), verbose=1)  # Increase epochs to 50\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "The code evaluates the trained model on the test data using the evaluate function. The test accuracy is printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 1s 5ms/step - loss: 1.2627 - accuracy: 0.6339\n",
      "Test accuracy: 63.39%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "def predict_next_actions(model, user_data, n_actions):\n",
    "    padding_length = model.input_shape[2] - user_data.shape[1]\n",
    "    user_data_padded = np.pad(user_data.values, ((0, 0), (padding_length, 0)), 'constant')\n",
    "    user_data_reshaped = user_data_padded.reshape((1, 1, user_data_padded.shape[1]))\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(n_actions):\n",
    "        prediction = model.predict(user_data_reshaped)\n",
    "        predictions.append(prediction)\n",
    "        prediction = prediction.reshape(1, 1, -1)  # Add an extra dimension to the prediction array\n",
    "        user_data_reshaped = np.concatenate((user_data_reshaped[:, :, :-prediction.shape[2]], prediction), axis=2)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Prediction Function for a Single User\n",
    "The code defines a function predict_next_actions to predict the next n actions for a given user. This function takes the trained model, user data, and the number of actions to predict as inputs. It pads the user data to the same shape as the input shape of the LSTM model, and then iteratively predicts the next action by concatenating the prediction to the end of the input data. The predicted actions are returned in encoded format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Next 5 actions for user 782347178622731989: [['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']]\n"
     ]
    }
   ],
   "source": [
    "user_id = 782347178622731989\n",
    "user_data = encoded_data[encoded_data['user_id_hashed'] == user_id].iloc[-1:].drop(columns=['user_id_hashed'])\n",
    "\n",
    "# Predict the next 5 actions for the given user\n",
    "predicted_actions_encoded = predict_next_actions(model, user_data, 10)\n",
    "\n",
    "# Decode the predicted actions\n",
    "predicted_actions = target_encoder.inverse_transform(predicted_actions_encoded.reshape(predicted_actions_encoded.shape[0], -1))\n",
    "print(f\"Next 5 actions for user {user_id}: {predicted_actions}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Function for Each Unique User\n",
    "The code defines a function predict_next_actions_for_each_user to predict the next n actions for each unique user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Next 5 actions for user 782347178622731989: [['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']]\n",
      "Next 5 actions for user 2911199505101500553: [['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']]\n",
      "Next 5 actions for user 3016752473480896665: [['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']]\n",
      "Next 5 actions for user 3709264465427925985: [['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']]\n",
      "Next 5 actions for user 4534275443679530850: [['completed_step']\n",
      " ['failed_submission']\n",
      " ['completed_step']\n",
      " ['failed_submission']\n",
      " ['completed_step']]\n",
      "Next 5 actions for user 6005434371571979741: [['failed_submission']\n",
      " ['failed_submission']\n",
      " ['failed_submission']\n",
      " ['failed_submission']\n",
      " ['failed_submission']]\n",
      "Next 5 actions for user 6440222428500388356: [['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']]\n",
      "Next 5 actions for user 6951787311528391747: [['failed_submission']\n",
      " ['failed_submission']\n",
      " ['failed_submission']\n",
      " ['failed_submission']\n",
      " ['failed_submission']]\n",
      "Next 5 actions for user 7065575396909049706: [['challengeSolved']\n",
      " ['challengeSolved']\n",
      " ['challengeSolved']\n",
      " ['challengeSolved']\n",
      " ['challengeSolved']]\n",
      "Next 5 actions for user 7163311985261360872: [['failed_submission']\n",
      " ['failed_submission']\n",
      " ['failed_submission']\n",
      " ['failed_submission']\n",
      " ['failed_submission']]\n",
      "Next 5 actions for user 7498222759795054055: [['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']\n",
      " ['codeChallengeSolved']]\n",
      "Next 5 actions for user 7554233681096237662: [['challengeSolved']\n",
      " ['challengeSolved']\n",
      " ['challengeSolved']\n",
      " ['challengeSolved']\n",
      " ['challengeSolved']]\n",
      "Next 5 actions for user 7956304405049214271: [['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']]\n",
      "Next 5 actions for user 8954549735489947820: [['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']\n",
      " ['completed_submission']]\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the next n actions for a given user data\n",
    "def predict_next_actions_for_each_user(model, encoded_data, n_actions=5):\n",
    "    unique_users = encoded_data['user_id_hashed'].unique()\n",
    "    predictions = {}\n",
    "\n",
    "    for user_id in unique_users:\n",
    "        user_data = encoded_data[encoded_data['user_id_hashed'] == user_id].iloc[-1:].drop(columns=['user_id_hashed'])\n",
    "        predicted_actions_encoded = predict_next_actions(model, user_data, n_actions)\n",
    "        predicted_actions = target_encoder.inverse_transform(predicted_actions_encoded.reshape(predicted_actions_encoded.shape[0], -1))\n",
    "        predictions[user_id] = predicted_actions\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Predict the next 5 actions for each unique user\n",
    "next_actions_for_each_user = predict_next_actions_for_each_user(model, encoded_data, 5)\n",
    "\n",
    "# Print the predictions\n",
    "for user_id, actions in next_actions_for_each_user.items():\n",
    "    print(f\"Next 5 actions for user {user_id}: {actions}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
